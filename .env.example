# ===========================================
# Atomic Inference - Environment Configuration
# ===========================================
# Copy this file to .env and fill in your API keys

# --- Cloud LLM Providers ---
OPENAI_API_KEY=sk-xxx
ANTHROPIC_API_KEY=sk-ant-xxx

# --- Local LLM - Ollama ---
OLLAMA_BASE_URL=http://localhost:11434

# --- OpenAI-Compatible API (LM Studio, vLLM, LocalAI, etc.) ---
# LiteLLM uses these for "openai/*" model prefix
OPENAI_API_BASE=http://localhost:1234/v1
# OPENAI_API_KEY=lm-studio  # Placeholder key for local servers (uncomment if needed)

# --- Default Model (used if not specified in code) ---
DEFAULT_MODEL=gpt-4o-mini
